{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.zeros((3**9, ))\n",
    "# compute a value for each of the states of the board. \n",
    "# This is kind of like using a minimax tree with depth 1, \n",
    "# for which the desirability of a future state is not computed explicitly depending upon actual wins and losses \n",
    "# but rather learned from past experience\n",
    "gamma = 0.9\n",
    "# discount factor: states that are close to the end of the game are rewarded/punished \n",
    "# more than states that are towards the beginning of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(board):\n",
    "    # hash function for boards, converts board to int so I can compute value of states\n",
    "    serial = 0\n",
    "    for i in range(9):\n",
    "        serial = serial + board[i]*3**(i)\n",
    "    return int(serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(serial):\n",
    "    # print board from seial number\n",
    "    board = np.zeros((9,))\n",
    "    for i in range(9):\n",
    "        board[i] = serial%3\n",
    "        serial = serial/3\n",
    "    print(board[0:3])\n",
    "    print(board[3:6])\n",
    "    print(board[6:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher(x):\n",
    "    # utility function, maps:\n",
    "    # even -> 1\n",
    "    # odd -> -1\n",
    "    return 1-2*(x%2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class game:\n",
    "    def __init__(self, training, generation=0):\n",
    "        self.board = np.zeros((9,))\n",
    "        # self.board is a 1D array representing the board state.\n",
    "        # 0 is empty, 1 is X, 2 is O\n",
    "\n",
    "        self.num_moves = 0\n",
    "        \n",
    "        self.history = []\n",
    "        # list of board states that are played during the game\n",
    "        \n",
    "        self.training = training\n",
    "        # is the game a \n",
    "        # training == 1: \"training\" game (values are being learned) or an \n",
    "        # training == 0: \"actual\" game (no learning; just play the best move)\n",
    "        \n",
    "        self.player = int(np.random.uniform()>0.5)\n",
    "        # if a training game, which player should play intelligently\n",
    "        \n",
    "        self.generation = generation\n",
    "        # generation number for value convergence. \n",
    "        # The weights I use (1/log(generation)) don't actually lead to convergence \n",
    "        # L2 norm of series should be finite for convergence\n",
    "        # but whatever\n",
    "        \n",
    "        \n",
    "    def clear_history(self):\n",
    "        # not used\n",
    "        self.board = np.zeros((9,))\n",
    "        self.history = []\n",
    "        self.num_moves = 0\n",
    "        \n",
    "    def make_move(self, move):\n",
    "        self.board[move] = self.num_moves%2+1\n",
    "\n",
    "        self.history += [serialize(self.board)]\n",
    "        # append board state to history, so that the value of the state can be updated at the end of the game\n",
    "        \n",
    "        self.num_moves = self.num_moves+1\n",
    "        \n",
    "    def game_won(self):\n",
    "        if (self.board[0] == self.board[1] and self.board[0] == self.board[2] and self.board[0] != 0) or \\\n",
    "            (self.board[0] == self.board[4] and self.board[0] == self.board[8] and self.board[0] != 0) or \\\n",
    "            (self.board[0] == self.board[3] and self.board[0] == self.board[6] and self.board[0] != 0) or \\\n",
    "            (self.board[1] == self.board[4] and self.board[1] == self.board[7] and self.board[1] != 0) or \\\n",
    "            (self.board[2] == self.board[4] and self.board[2] == self.board[6] and self.board[2] != 0) or \\\n",
    "            (self.board[2] == self.board[5] and self.board[2] == self.board[8] and self.board[2] != 0) or \\\n",
    "            (self.board[3] == self.board[4] and self.board[3] == self.board[5] and self.board[3] != 0) or \\\n",
    "            (self.board[6] == self.board[7] and self.board[6] == self.board[8] and self.board[6] != 0):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def legal_moves(self):\n",
    "        # returns a list of legal moves\n",
    "        moves = []\n",
    "        for i in range(9):\n",
    "            if self.board[i] == 0:\n",
    "                moves += [i]\n",
    "        return moves\n",
    "    \n",
    "    def get_move(self):\n",
    "        moves = self.legal_moves()\n",
    "        vs = np.zeros((len(moves),))\n",
    "        potential_boards = [self.board.copy() for i in range(len(moves))]\n",
    "        # list of potential boards, one for each legal move\n",
    "\n",
    "        for ind, m in enumerate(moves):\n",
    "            potential_boards[ind][m] = self.num_moves%2+1\n",
    "            vs[ind] = values[serialize(potential_boards[ind])]\n",
    "            # get the values of these boards. \n",
    "            # Value of a state indicates the goodness of leaving the board in that state after your turn, \n",
    "            # so you want to play in such a way as to maximize the value of the state of the potential board   \n",
    "            \n",
    "        if self.training:\n",
    "            # if training, play randomly on every other move\n",
    "            if self.player == self.num_moves%2:\n",
    "                return moves[np.argmax(np.random.multinomial(1, softmax(vs)))]\n",
    "                # use boltzmann style distribution to compute next move \n",
    "                # plays less optimal moves with some probability to encourage exploration\n",
    "            else:\n",
    "                return random.sample(moves, 1)   \n",
    "        else:\n",
    "            for ind, m in enumerate(moves):\n",
    "                print(potential_boards[ind][0:3])\n",
    "                print(potential_boards[ind][3:6])\n",
    "                print(potential_boards[ind][6:9])\n",
    "                print(vs[ind])\n",
    "            return moves[np.argmax(vs)]\n",
    "            # if actual game, just play the best move\n",
    "            \n",
    "    \n",
    "    def update_values(self):\n",
    "        winner = 2-(self.num_moves%2) # 1 if p1 won, 2 if p2 won\n",
    "        for ind, b in enumerate(list(reversed(self.history))):\n",
    "            values[b] = values[b]+rademacher(ind)*gamma**ind/np.log(self.generation+1)\n",
    "            # increases value if it leads to \"game over\" after an even number of moves, \n",
    "            # decreases value if it leads to \"game over\" after an off number of moves\n",
    "            \n",
    "    def print_game(self):\n",
    "        print(self.board[0:3])\n",
    "        print(self.board[3:6])\n",
    "        print(self.board[6:9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(generation):\n",
    "    training_game = game(1, generation)\n",
    "    while training_game.num_moves < 9 and not training_game.game_won():\n",
    "        training_game.make_move(training_game.get_move())\n",
    "    #training_game.print_game()\n",
    "    if training_game.game_won():\n",
    "        training_game.update_values()\n",
    "        #print(serialize(training_game.board))\n",
    "    #training_game.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 100000):\n",
    "    play_game(i)\n",
    "#     if winner == 1:\n",
    "#         print('p1 won')\n",
    "#     elif winner == 2:\n",
    "#         print('p2 won')\n",
    "#     else:\n",
    "#         print('draw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 1.]\n",
      "[1. 2. 0.]\n",
      "[0. 0. 2.]\n",
      "50.000690490708465\n"
     ]
    }
   ],
   "source": [
    "v = 13325\n",
    "print_board(v)\n",
    "print(values[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 to play first, 1 to play second: 1\n",
      "[1. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "-220.97841597482147\n",
      "[0. 1. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "-227.83757416467014\n",
      "[0. 0. 1.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "-212.2125616164353\n",
      "[0. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "-233.00841083086186\n",
      "[0. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 0.]\n",
      "-89.57149665940086\n",
      "[0. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 0.]\n",
      "-232.5393812592532\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "2519.6928912136386\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "-230.35630710484548\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "-209.78100593972758\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "play_move: 4\n",
      "[0. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 0. 0.]\n",
      "-83.5332795790565\n",
      "[0. 1. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 0. 0.]\n",
      "-77.33220086188193\n",
      "[0. 0. 1.]\n",
      "[0. 2. 0.]\n",
      "[1. 0. 0.]\n",
      "-90.26478496354996\n",
      "[0. 0. 0.]\n",
      "[1. 2. 0.]\n",
      "[1. 0. 0.]\n",
      "-88.54748303361816\n",
      "[0. 0. 0.]\n",
      "[0. 2. 1.]\n",
      "[1. 0. 0.]\n",
      "-86.99105987728035\n",
      "[0. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 1. 0.]\n",
      "328.18158731842766\n",
      "[0. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 0. 1.]\n",
      "-77.91686620164704\n",
      "[0. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 1. 0.]\n",
      "play_move: 8\n",
      "[0. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "[1. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "40.726370298749025\n",
      "[0. 1. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "-26.857646250694476\n",
      "[0. 0. 1.]\n",
      "[0. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "-25.653976491699012\n",
      "[0. 0. 0.]\n",
      "[1. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "-23.603776692199737\n",
      "[0. 0. 0.]\n",
      "[0. 2. 1.]\n",
      "[1. 1. 2.]\n",
      "-26.26288934489905\n",
      "[1. 0. 0.]\n",
      "[0. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "play_move: 3\n",
      "[1. 0. 0.]\n",
      "[2. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "[1. 1. 0.]\n",
      "[2. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "-10.283095318184618\n",
      "[1. 0. 1.]\n",
      "[2. 2. 0.]\n",
      "[1. 1. 2.]\n",
      "-9.704361704923128\n",
      "[1. 0. 0.]\n",
      "[2. 2. 1.]\n",
      "[1. 1. 2.]\n",
      "0.0\n",
      "[1. 0. 0.]\n",
      "[2. 2. 1.]\n",
      "[1. 1. 2.]\n",
      "play_move: 2\n",
      "[1. 0. 2.]\n",
      "[2. 2. 1.]\n",
      "[1. 1. 2.]\n",
      "[1. 1. 2.]\n",
      "[2. 2. 1.]\n",
      "[1. 1. 2.]\n",
      "0.0\n",
      "[1. 1. 2.]\n",
      "[2. 2. 1.]\n",
      "[1. 1. 2.]\n",
      "tie\n"
     ]
    }
   ],
   "source": [
    "role = input(\"0 to play first, 1 to play second: \")\n",
    "bot_turn = 1-role\n",
    "actual_game = game(0)\n",
    "turn = 0\n",
    "while actual_game.num_moves < 9 and not actual_game.game_won():\n",
    "    if not turn == bot_turn:\n",
    "        move = input(\"play_move: \")\n",
    "        actual_game.make_move(move)\n",
    "    else:\n",
    "        actual_game.make_move(actual_game.get_move())\n",
    "    actual_game.print_game()\n",
    "    turn = 1-turn\n",
    "\n",
    "if not actual_game.game_won() and actual_game.num_moves == 9:\n",
    "    print('tie')\n",
    "if actual_game.game_won() and actual_game.num_moves%2 == bot_turn-1:\n",
    "    print('you lost')\n",
    "if actual_game.game_won() and actual_game.num_moves%2 == bot_turn:\n",
    "    print('you won')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
